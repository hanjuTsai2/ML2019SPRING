{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the training data and import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/train.csv', encoding='Big5')\n",
    "data = data.replace('NR', '0')\n",
    "data = np.array(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "hour = 9\n",
    "feature_num = 18\n",
    "day_per_month = 20\n",
    "per_month_row = feature_num * day_per_month\n",
    "total_month = int(len(data)/per_month_row) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the data 9 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_data = []\n",
    "for i in range(total_month):\n",
    "    l = data[i * per_month_row: i * per_month_row  + per_month_row]\n",
    "    month_data.append(l)  \n",
    "\n",
    "hour_data = []\n",
    "for i in range(len(month_data)):\n",
    "    tmp = []\n",
    "    for j in range(len(month_data[i])):\n",
    "        tmp.append(month_data[i][j])\n",
    "    hour_data.append(tmp)\n",
    "    \n",
    "total = []\n",
    "for i in range(len(hour_data)):\n",
    "    df = pd.DataFrame(hour_data[i])\n",
    "    row, col = df.shape\n",
    "    tmp = None\n",
    "    for j in range(day_per_month):\n",
    "        per_day = df.iloc[j*feature_num:j*feature_num+feature_num,:]\n",
    "        per_day = per_day.iloc[:,3:]\n",
    "        \n",
    "        if tmp is not None:\n",
    "            tmp = pd.concat([tmp.reset_index(drop=True), per_day.reset_index(drop=True)], axis=1, ignore_index=True)\n",
    "        else:\n",
    "            tmp = pd.DataFrame(per_day)   \n",
    "     \n",
    "    total.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "xss = []\n",
    "yss = []\n",
    "for i in range(len(total)):\n",
    "    df = total[i]\n",
    "    row, col = df.shape\n",
    "    for j in range(col-hour-1):\n",
    "        xs = df.iloc[:,j:j+hour]\n",
    "        xs = xs.values.ravel()\n",
    "        ys = float(df.iloc[9,j+hour])\n",
    "        xss.append(xs)\n",
    "        if(ys < 0):\n",
    "            yss.append(0)\n",
    "            continue         \n",
    "        yss.append(float(ys))\n",
    "        \n",
    "xss = pd.DataFrame(xss).values.astype(np.float)\n",
    "#np.column_stack(([1] * len(xss) ,xss)) ## if ho feature scaling\n",
    "yss = np.array(yss)\n",
    "ori_xss = xss\n",
    "ori_yss = yss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q75, q25 = np.percentile(yss, [75 ,25])\n",
    "iqr = q75 - q25\n",
    "lb = q25 - 1.5 * iqr\n",
    "ub = q75 + 2 * iqr\n",
    "model = list(zip(xss,yss))\n",
    "model = [y for y in model if y[1] <= ub and y[1] >= lb]\n",
    "xss = [y[0] for y in model]\n",
    "yss = [y[1] for y in model]\n",
    "print(len(ori_yss)-len(yss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "corr = X_train.corr()\n",
    "sns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True), square=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class standardScaler():\n",
    "    def fit(self, xss):\n",
    "        self.mean = np.mean(xss, axis=0)\n",
    "        self.sd = np.std(xss, axis=0)\n",
    "\n",
    "    def transform(self, xss):\n",
    "        xss = (xss-self.mean)/(self.sd)\n",
    "        return(xss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = standardScaler()\n",
    "scaler.fit(xss)\n",
    "xss = scaler.transform(xss)\n",
    "row, col = xss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X_train\n",
    "vif = pd.DataFrame()\n",
    "vif[\"VIF Factor\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "vif[\"features\"] = X.columns\n",
    "vec = [i for i in vif['VIF Factor'] if i > 70]\n",
    "print(len(vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor    \n",
    "def calculate_vif_(X, thresh=70):\n",
    "    cols = X.columns\n",
    "    variables = np.arange(X.shape[1])\n",
    "    dropped=True\n",
    "    while dropped:\n",
    "        dropped=False\n",
    "        c = X[cols[variables]].values\n",
    "        vif = [variance_inflation_factor(c, ix) for ix in np.arange(c.shape[1])]\n",
    "\n",
    "        maxloc = vif.index(max(vif))\n",
    "        if max(vif) > thresh:\n",
    "            variables = np.delete(variables, maxloc)\n",
    "            dropped=True\n",
    "\n",
    "    print('Remaining variables:')\n",
    "    print(X.columns[variables])\n",
    "    return (X.columns[variables], X[cols[variables]])\n",
    "\n",
    "X_train = pd.DataFrame(xss)\n",
    "res = calculate_vif_(X_train)\n",
    "selected_var , xss = res[0], res[1].values\n",
    "xss = np.column_stack(([1] * len(xss) ,xss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model. Don't forget to remove the clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107\n",
      "5.67245802343452\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-5d05db7a37bc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mpredict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mw_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myss\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0mw_lr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw_lr\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mw_grad\u001b[0m \u001b[1;33m**\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw_lr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mw_grad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "xss = np.column_stack(([1] * len(xss) ,xss))\n",
    "from IPython.display import clear_output\n",
    "max_iter = 10 ** 4\n",
    "epochs = 10 ** 3\n",
    "lr = 1\n",
    "num = xss.shape[1]\n",
    "w = np.zeros(num)\n",
    "w_lr = np.zeros(num)\n",
    "\n",
    "for t in range(epochs):\n",
    "    w_grad = None\n",
    "    for m in range(max_iter):\n",
    "        predict = np.dot(xss,w)\n",
    "        w_grad = -(2 * np.dot(xss.T,(yss - predict)))\n",
    "        w_lr = w_lr + w_grad ** 2\n",
    "        w = w - lr/np.sqrt(w_lr) * w_grad\n",
    "        \n",
    "    clear_output()\n",
    "    print(t)\n",
    "    print(np.sqrt(np.mean([ x*x for x in (yss-predict)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_xss = scaler.transform(ori_xss)\n",
    "ori_xss = np.column_stack(([1] * len(ori_xss) ,ori_xss))\n",
    "predict = np.dot(ori_xss,w)\n",
    "print(np.sqrt(np.mean([ x*x for x in (ori_yss-predict)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('data/test.csv', encoding='Big5', header=None)\n",
    "test = test.replace('NR', '0')\n",
    "file = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "dic = {'w': w, 'scaler': scaler}\n",
    "with open('model/'+file+'.pkl', 'wb') as f:\n",
    "    pickle.dump(dic, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240\n"
     ]
    }
   ],
   "source": [
    "ans = pd.read_csv('data/sampleSubmission.csv', encoding='Big5')\n",
    "test_feature = 18\n",
    "total_test = []\n",
    "row, col = test.shape\n",
    "test_number = int(row/test_feature)\n",
    "print(test_number)\n",
    "for i in range(test_number):\n",
    "    df = test.iloc[i*test_feature:(i+1)*test_feature, 2:]\n",
    "    xs = df.values.ravel().astype(np.float)\n",
    "    xs = (scaler.transform([xs])[0])\n",
    "#     xs = [xs[i] for i in range(len(xs)) if i in selected_var]\n",
    "    xs = np.concatenate(([1], xs))\n",
    "    val = np.dot(xs,w)\n",
    "    val = max(val,0)\n",
    "    ans.iloc[i,1] = val\n",
    "ans.to_csv('data/'+file+'.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_var.values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
