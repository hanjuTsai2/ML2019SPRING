{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the training data and import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class standardScaler():\n",
    "    def fit(self, xss):\n",
    "        self.mean = np.mean(xss, axis=0)\n",
    "        self.sd = np.std(xss, axis=0)\n",
    "\n",
    "    def transform(self, xss):\n",
    "        xss = (xss-self.mean)/(self.sd)\n",
    "        return(xss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures \n",
    "def polyTransform(xss):\n",
    "    xss = np.column_stack([xss, xss**2])\n",
    "    return(xss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the data 9 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_Initial_Parameters(outliers=[], scale=True):\n",
    "    # read data\n",
    "    data = pd.read_csv('data/train.csv', encoding='Big5')\n",
    "    data = data.replace('NR', '0')\n",
    "    data = np.array(data)\n",
    "    \n",
    "    # feature variable\n",
    "    hour = 9\n",
    "    feature_num = 18\n",
    "    day_per_month = 20\n",
    "    per_month_row = feature_num * day_per_month\n",
    "    total_month = int(len(data)/per_month_row) \n",
    "    \n",
    "    month_data = []\n",
    "    for i in range(total_month):\n",
    "        l = data[i * per_month_row: i * per_month_row  + per_month_row]\n",
    "        month_data.append(l)  \n",
    "\n",
    "    hour_data = []\n",
    "    for i in range(len(month_data)):\n",
    "        tmp = []\n",
    "        for j in range(len(month_data[i])):\n",
    "            tmp.append(month_data[i][j])\n",
    "        hour_data.append(tmp)\n",
    "\n",
    "    total = []\n",
    "    for i in range(len(hour_data)):\n",
    "        df = pd.DataFrame(hour_data[i])\n",
    "        row, col = df.shape\n",
    "        tmp = None\n",
    "        for j in range(day_per_month):\n",
    "            per_day = df.iloc[j*feature_num:j*feature_num+feature_num,:]\n",
    "            per_day = per_day.iloc[:,3:]\n",
    "\n",
    "            if tmp is not None:\n",
    "                tmp = pd.concat([tmp.reset_index(drop=True), per_day.reset_index(drop=True)], axis=1, ignore_index=True)\n",
    "            else:\n",
    "                tmp = pd.DataFrame(per_day)   \n",
    "\n",
    "        total.append(tmp)\n",
    "        \n",
    "    xss = []\n",
    "    yss = []\n",
    "    ori_xss = []\n",
    "    ori_yss = []\n",
    "\n",
    "    for i in range(len(total)):\n",
    "        df = total[i]\n",
    "        row, col = df.shape\n",
    "        for j in range(col-hour):\n",
    "            xs = df.iloc[:,j:j+hour]\n",
    "            xs = xs.values.ravel()\n",
    "            ys = float(df.iloc[9,j+hour])\n",
    "\n",
    "            ori_xss.append(xs)\n",
    "            ori_yss.append(float(ys))\n",
    "\n",
    "            if(ys < 0):\n",
    "                continue\n",
    "                \n",
    "            xss.append(xs)\n",
    "            yss.append(float(ys))\n",
    "\n",
    "    xss = pd.DataFrame(xss).values.astype(np.float)\n",
    "    yss = np.array(yss)\n",
    "    ori_xss = pd.DataFrame(ori_xss).values.astype(np.float)\n",
    "    ori_yss = np.array(ori_yss)\n",
    "\n",
    "    if scale:\n",
    "        ## feature scaling\n",
    "        scaler = standardScaler()\n",
    "        scaler.fit(xss)\n",
    "        xss = scaler.transform(xss)\n",
    "        row, col = xss.shape\n",
    "    \n",
    "    if outliers:\n",
    "        xss = np.array([ xss[i] for i in range(len(xss)) if i not in outlier])\n",
    "        yss = np.array([ yss[i] for i in range(len(yss)) if i not in outlier])\n",
    "        \n",
    "    return ({'xss': xss, 'yss': yss, 'ori_xss': ori_xss, 'ori_yss': ori_yss, 'scaler': scaler})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor    \n",
    "def calculate_vif_(X, thresh=70):\n",
    "    cols = X.columns\n",
    "    variables = np.arange(X.shape[1])\n",
    "    dropped=True\n",
    "    while dropped:\n",
    "        dropped=False\n",
    "        c = X[cols[variables]].values\n",
    "        vif = [variance_inflation_factor(c, ix) for ix in np.arange(c.shape[1])]\n",
    "\n",
    "        maxloc = vif.index(max(vif))\n",
    "        if max(vif) > thresh:\n",
    "            variables = np.delete(variables, maxloc)\n",
    "            dropped=True\n",
    "\n",
    "    print('Remaining variables:')\n",
    "    print(X.columns[variables])\n",
    "    return (X.columns[variables], X[cols[variables]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = generate_Initial_Parameters(outlier)\n",
    "xss = param['xss']\n",
    "yss = param['yss']\n",
    "scaler = param['scaler']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xss = polyTransform(xss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = calculate_vif_(pd.DataFrame(xss),80)\n",
    "selected_var = res[0]\n",
    "print(selected_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select only past pm2.5 as factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_var = np.array([  0,   2,   4,   7,   8,   9,  10,  11,  12,  13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,  28,  29, 30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,  42, 43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  63,  64,  65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77, 78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90, 91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,  156, 157, 158, 159, 160, 161]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_var = np.array([(i)*feature_num +9 for i in range(9)]\n",
    "X_train = pd.DataFrame(xss)\n",
    "X_train = X_train[selected_var]\n",
    "xss = X_train.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(xss, yss, test_size=0.2, random_state=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model. Don't forget to remove the clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "def gradientDescent(xss, yss , alpha):\n",
    "    lr = 1\n",
    "    max_iter = 10 ** 3\n",
    "    epochs = 10 ** 3\n",
    "    \n",
    "    ## get bias\n",
    "    xss = np.column_stack(([1] * len(xss) ,xss))\n",
    "    num = xss.shape[1]\n",
    "    w = np.zeros(num)\n",
    "    w_lr = np.zeros(num)\n",
    "    \n",
    "    for t in range(epochs):\n",
    "        w_grad = None\n",
    "        for m in range(max_iter):\n",
    "            predict = np.dot(xss,w)\n",
    "            w_grad = -(2 * np.dot(xss.T,(yss - predict))) + (alpha * np.sum(w**2) * 2)\n",
    "            w_lr = w_lr + w_grad ** 2\n",
    "            w = w - lr/np.sqrt(w_lr) * w_grad\n",
    "\n",
    "        clear_output()\n",
    "        print(np.sqrt(np.mean([ x*x for x in (yss-predict)])))  \n",
    "    return (w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_list = [10, 6, 5, 4, 3, 2, 1, 0.5, 0.3, 0.2, 0.1, 0.01, 0.001, 0.0001]\n",
    "alpha_list = np.logspace(-5, 5, 5)\n",
    "alpha_list = [0]\n",
    "total_his = []\n",
    "\n",
    "param = generate_Initial_Parameters()\n",
    "tmp = param['xss']\n",
    "yss = param['yss']\n",
    "scaler = param['scaler']\n",
    "\n",
    "for i in range(10,150,10):\n",
    "    his = []\n",
    "    res = calculate_vif_(pd.DataFrame(xss),i)\n",
    "    selected_var = res[0]\n",
    "    X_train = pd.DataFrame(tmp)\n",
    "    X_train = X_train[selected_var]\n",
    "    xss = X_train.values\n",
    "    for seed in range(20,90,10):\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(xss, yss, test_size=0.2, random_state=seed)\n",
    "\n",
    "        alpha = 0\n",
    "        w = gradientDescent(X_train, Y_train, alpha)\n",
    "        xss_tmp = np.column_stack(([1] * len(X_test) ,X_test))\n",
    "        yss_tmp = Y_test\n",
    "        predict = np.dot(xss_tmp ,w)\n",
    "        rmse = (np.sqrt(np.mean([ x*x for x in (yss_tmp-predict)])))\n",
    "        his.append(rmse)\n",
    "\n",
    "        print(his)\n",
    "        print(his.index(min(his)))\n",
    "        \n",
    "    total_his.append(his)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = calculate_vif_(pd.DataFrame(xss),80)\n",
    "selected_var = res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = gradientDescent(xss, yss, alpha = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('data/test.csv', encoding='Big5', header=None)\n",
    "test = test.replace('NR', '0')\n",
    "file = '0306-6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "dic = {'w': w, 'scaler': scaler, 'selected_var' : selected_var, 'alpha' : alpha}\n",
    "with open('model/'+file+'.pkl', 'wb') as f:\n",
    "    pickle.dump(dic, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = dic['w']\n",
    "scaler = dic['scaler']\n",
    "selected_var = dic['selected_var']\n",
    "\n",
    "ans = pd.read_csv('data/sampleSubmission.csv', encoding='Big5')\n",
    "test_feature = 18\n",
    "total_test = []\n",
    "row, col = test.shape\n",
    "test_number = int(row/test_feature)\n",
    "\n",
    "for i in range(test_number):\n",
    "    df = test.iloc[i*test_feature:(i+1)*test_feature, 2:]\n",
    "    xs = df.values.ravel().astype(np.float)\n",
    "    xs = (scaler.transform([xs])[0])\n",
    "    xs = np.array([[xs[i] for i in range(len(xs)) if i in selected_var]])\n",
    "    xs = polyTransform(xs)[0]\n",
    "    xs = np.concatenate(([1], xs))\n",
    "    val = np.dot(xs,w)\n",
    "    ans.iloc[i,1] = val\n",
    "ans.to_csv('data/'+file+'.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "his = []\n",
    "his_train = []\n",
    "alphas = np.logspace(-5, 5, 5)\n",
    "min_rmse = 10000\n",
    "for m in range(len(alphas)):\n",
    "    for k in range(10,50,5):\n",
    "        alpha = alphas[m]\n",
    "        if k % 10 == 0:\n",
    "            clear_output()\n",
    "            \n",
    "        ## take all \n",
    "        param = generate_Initial_Parameters()\n",
    "        xss = param['xss']\n",
    "        yss = param['yss']\n",
    "        xss = xss[:,selected_var]\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(xss, yss, test_size=0.2, random_state=28)\n",
    "        w = gradientDescent(X_train, Y_train, alpha = alpha)\n",
    "        \n",
    "        val = np.column_stack(([1] * len(X_train) ,X_train))\n",
    "        predict = np.dot(val,w)\n",
    "        predict = np.array([round(i,0) for i in predict])\n",
    "        Y_train = np.array(Y_train)\n",
    "\n",
    "        error = (predict-Y_train)\n",
    "        rmse = np.sqrt(np.mean(error**2))\n",
    "        his_train.append(rmse)\n",
    "        print('training rmse =', rmse, end=',')\n",
    "        \n",
    "        ## take the outliers out\n",
    "        outlier = [ i for i in range(len(error)) if abs(error[i]) >= k]\n",
    "        X_train = [ X_train[i] for i in range(len(X_train)) if i not in outlier]\n",
    "        Y_train = [ Y_train[i] for i in range(len(Y_train)) if i not in outlier]\n",
    "        \n",
    "        w = gradientDescent(X_train, Y_train, alpha = alpha)\n",
    "        val = np.column_stack(([1] * len(X_test) ,X_test))\n",
    "        predict = np.dot(val,w)\n",
    "        predict = np.array([round(i,0) for i in predict])\n",
    "        Y_test = np.array(Y_test)\n",
    "        \n",
    "        error = (predict-Y_test)\n",
    "        rmse = np.sqrt(np.mean(error**2))\n",
    "        \n",
    "        min_rmse = min(rmse, min_rmse)\n",
    "        if min_rmse == rmse:\n",
    "            res['w'] = w\n",
    "            res['alpha'] = alpha\n",
    "            res['k'] = k\n",
    "            \n",
    "        print('rmse =', rmse)\n",
    "        his.append(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = np.dot(np.column_stack(([1] * len(xss) ,xss)),w) - yss \n",
    "plt.plot(error)\n",
    "outlier =  [ i for i in range(len(error)) if abs(error[i]) >= 100]\n",
    "print(outlier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=res['k']\n",
    "al=res['alpha']\n",
    "w = res['w']\n",
    "\n",
    "param = generate_Initial_Parameters(outliers=[])\n",
    "xss = param['xss']\n",
    "xss = xss[:,selected_var]\n",
    "xss = polyTransform(xss)\n",
    "yss = param['yss']\n",
    "scaler = param['scaler']\n",
    "\n",
    "val = np.column_stack([([1] * len(xss)) , xss])\n",
    "predict = np.dot(val,w)\n",
    "predict = np.array([round(i,0) for i in predict])\n",
    "yss = np.array(yss)\n",
    "error = (predict-yss)\n",
    "outlier = [ i for i in range(len(error)) if abs(error[i]) >= k]\n",
    "print(len(outlier))\n",
    "\n",
    "param = generate_Initial_Parameters(outliers=outlier)\n",
    "xss = param['xss']\n",
    "xss = xss[:,selected_var]\n",
    "yss = param['yss']\n",
    "scaler = param['scaler']\n",
    "\n",
    "val = np.column_stack(([1] * len(xss) ,xss))\n",
    "predict = np.dot(val,w)\n",
    "predict = np.array([round(i,0) for i in predict])\n",
    "yss = np.array(yss)\n",
    "w = gradientDescent(xss, yss , alpha = 10)\n",
    "print(len(w))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
